{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kr/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is a large dataset of handwritten numerical digits. More about the dataset: https://en.wikipedia.org/wiki/MNIST_database\n",
    "        \n",
    "\n",
    "The digits in the images are centred and scaled (similar sized).\n",
    "We proceed by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  # Load training and eval data\n",
    "    mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "    train_data = mnist.train.images # Returns np.array\n",
    "    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "    test_data = mnist.test.images # Returns np.array\n",
    "    test_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kr/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-4-1ecfc4c651cc>:3: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data.\n",
      "WARNING:tensorflow:From /home/kr/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/kr/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/kr/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/kr/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/kr/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/kr/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels, test_data, test_labels = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the size of the training data?\n",
    "# train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualise a datapoint, an image in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd202c712b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "some_digit = train_data[2]\n",
    "some_digit_image = some_digit.reshape(28,28) # reshape to square\n",
    "plt.imshow(some_digit_image,cmap = plt.cm.binary,interpolation=\"nearest\")\n",
    "# plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms that coud be used for the problem:\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Random Forest\n",
    "\n",
    "SVM\n",
    "\n",
    "k-Nearest Neighbours\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "n. Neural Networks- Feed-forward, Convolutional, Dense Convolutional, RNNs \n",
    "\n",
    "But which one?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Some alterations that could be done to an image so as to check the effects of learning:\n",
    "\n",
    "b = np.zeros([28,28], float)\n",
    "\n",
    "#1. resizing the digit in the image to its 1/4th size and shifting it to the top left quadrant and rotating it.\n",
    "for i in range(14):\n",
    "    for j in range(14):\n",
    "        p = some_digit_image[2*i][2*j]\n",
    "        b[14-j][i] = p\n",
    "\n",
    "#2. resizing the digit in the image to its 1/4th size and shifting it to the top left quadrant\n",
    "for i in range(14):\n",
    "    for j in range(14):\n",
    "        p = some_digit_image[2*i][2*j]\n",
    "        b[i][j] = p\n",
    "\n",
    "#3. rotation\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        p = some_digit_image[i][j]\n",
    "        b[27-j][i] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd202c71080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = some_digit_image\n",
    "\n",
    "plt.imshow(b,cmap = plt.cm.binary,interpolation=\"nearest\")\n",
    "# plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Convolutional neural network the neural network structure is\n",
    "\n",
    "    Input Layer\n",
    "    Covolution layer (5*5 feature detector, 32 output)\n",
    "    2*2 max pooling\n",
    "    Covolution layer (5*5 feature detector, 64 output)\n",
    "    2*2 max pooling\n",
    "    full connect layers : say 1 layer for now.\n",
    "    dropout layer\n",
    "    softmax the outcome\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to process our labels in a one-hot encoding so that our predictions that would be 'probabilities' across the range of digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_reshaped = train_labels.reshape(-1,1)\n",
    "test_labels_reshaped = test_labels.reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "train_L = enc.fit_transform(train_labels_reshaped).toarray()\n",
    "test_L = enc.transform(test_labels_reshaped).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-ab019b1b82e9>:44: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    X = tf.placeholder(tf.float32, [None,784])\n",
    "    Y = tf.placeholder(tf.float32,[None,10])\n",
    "    X_ = tf.reshape(X,[-1,28,28,1]) \n",
    "    # for convolution. This operation would not be required if we plan to use just a feed-fwd network\n",
    "    \n",
    "    dropout_prob = tf.placeholder(tf.float32)\n",
    "    #dropout is a technique to avoid overfitting. A probability parameter is required for the same.\n",
    "    #arguments are passed in the computational graph via a placeholder.\n",
    "    #So far, X, Y, dropout_prob make use of the  placeholder.\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    \n",
    "    F1 = 32 #num of filters/kernels/convolutions in 1st layer\n",
    "    F2 = 64 #num of filters/kernels/convolutions in 2nd layer\n",
    "\n",
    "    \n",
    "    C1 = tf.Variable(tf.truncated_normal([5,5,1,F1], 0, 0.1, tf.float32))\n",
    "    B1 = tf.Variable(tf.ones([F1])/10)\n",
    "    \n",
    "    C2 = tf.Variable(tf.truncated_normal([5,5,F1,F2], 0, 0.1, tf.float32))\n",
    "    B2 = tf.Variable(tf.ones([F2])/10)\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([7*7*F2,10], 0, 0.1, tf.float32))\n",
    "    B3 = tf.Variable(tf.ones([10])/10)\n",
    "\n",
    "   \n",
    "    #stride of 1 for both convolutions\n",
    "    L1 = tf.nn.relu(tf.nn.conv2d(X_, C1, strides=[1,1,1,1], padding='SAME') + B1)\n",
    "    L1_ = tf.nn.max_pool(L1,ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    L2 = tf.nn.relu(tf.nn.conv2d(L1_, C2, strides=[1,1,1,1], padding='SAME') + B2)\n",
    "    L2_ = tf.nn.max_pool(L2, ksize = [1,2,2,1], strides = [1,2,2,1], padding='SAME')\n",
    "    \n",
    "    L3 = tf.reshape(L2_,shape=[-1,7*7*F2])\n",
    "    \n",
    "    #################1 feed fwd layer. No activation since the last layer.\n",
    "    L4 = tf.matmul(L3, W3) + B3\n",
    "    L = tf.nn.softmax(L4)    \n",
    "    #for each data-point, L would be a 10-sized array of estimated probabilities for each digit\n",
    "    #################\n",
    "\n",
    "    #LOSS\n",
    "    crossE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=L4, labels=Y))\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(L,1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    train_step = optimizer.minimize(crossE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = np.reshape(b, [784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:  0.004\n",
      "Epoch1\n",
      "ACCURACY:  0.794\n",
      "LR:  0.003062331280577377\n",
      "Epoch2\n",
      "ACCURACY:  0.7607\n",
      "LR:  0.002350104260483898\n",
      "Epoch3\n",
      "ACCURACY:  0.8521\n",
      "LR:  0.0018091164706133022\n",
      "Epoch4\n",
      "ACCURACY:  0.8786\n",
      "LR:  0.0013981972264225104\n",
      "Epoch5\n",
      "ACCURACY:  0.8898\n",
      "LR:  0.0010860744236385112\n",
      "Epoch6\n",
      "ACCURACY:  0.8873\n",
      "LR:  0.0008489946436209412\n",
      "Epoch7\n",
      "ACCURACY:  0.9022\n",
      "LR:  0.0006689154517392868\n",
      "Epoch8\n",
      "ACCURACY:  0.9163\n",
      "LR:  0.0005321323176131021\n",
      "Epoch9\n",
      "ACCURACY:  0.9137\n",
      "LR:  0.0004282356620035104\n",
      "Epoch10\n",
      "ACCURACY:  0.9295\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# check_data = train_data[:5]\n",
    "# check_L = train_L[:5]\n",
    "# bb = tf.reshape(b, shape = [784])\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    builder = tf.saved_model.builder.SavedModelBuilder()\n",
    "    \n",
    "    batch_size = 100\n",
    "    epochs = 10\n",
    "    iterations = int(train_data.shape[0]/batch_size)\n",
    "    batch_num = 0\n",
    "    \n",
    "    max_learning_rate = 0.004\n",
    "    min_learning_rate = 0.0001\n",
    "    decay_speed = 2000.0\n",
    "    learning_rate = max_learning_rate\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(\"LR: \",learning_rate)\n",
    "        for j in range(iterations):\n",
    "            batch_num +=1\n",
    "            start_idx = (i * batch_size) % (train_data.shape[0] - batch_size)\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            x_train = train_data[start_idx:end_idx]\n",
    "            y_train = train_L[start_idx:end_idx]\n",
    "            \n",
    "            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-batch_num/decay_speed)\n",
    "\n",
    "            x = sess.run(train_step, feed_dict = {X:x_train, Y:y_train, dropout_prob: 0.5, lr: learning_rate})\n",
    "        print (\"Epoch\"+str(i+1))\n",
    "#         print (\"batch: \"+ str(batch_num+1))\n",
    "        ans=sess.run(accuracy,feed_dict={X:test_data,Y:test_L,dropout_prob:1,lr:learning_rate}) # evaluate the testing dataset.\n",
    "        print(\"ACCURACY: \", ans)\n",
    "#         ans2 = sess.run(L1_, feed_dict = {X:[bc], Y: check_L[2:3], dropout_prob:1, lr: learning_rate})\n",
    "#         checks = sess.run(L, feed_dict={X:check_data, Y: check_L, dropout_prob:1,lr: learning_rate})\n",
    "#         print (checks)\n",
    "#         check = sess.run(L6, feed_dict={X:[bc], Y: check_L[2:3], dropout_prob:1,lr: learning_rate})\n",
    "#         print (check)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'Placeholder' type=Placeholder>,\n",
       " <tf.Operation 'Placeholder_1' type=Placeholder>,\n",
       " <tf.Operation 'Reshape/shape' type=Const>,\n",
       " <tf.Operation 'Reshape' type=Reshape>,\n",
       " <tf.Operation 'Placeholder_2' type=Placeholder>,\n",
       " <tf.Operation 'Placeholder_3' type=Placeholder>,\n",
       " <tf.Operation 'truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'truncated_normal' type=Add>,\n",
       " <tf.Operation 'Variable' type=VariableV2>,\n",
       " <tf.Operation 'Variable/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable/read' type=Identity>,\n",
       " <tf.Operation 'ones/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'ones/Const' type=Const>,\n",
       " <tf.Operation 'ones' type=Fill>,\n",
       " <tf.Operation 'truediv/y' type=Const>,\n",
       " <tf.Operation 'truediv' type=RealDiv>,\n",
       " <tf.Operation 'Variable_1' type=VariableV2>,\n",
       " <tf.Operation 'Variable_1/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_1/read' type=Identity>,\n",
       " <tf.Operation 'truncated_normal_1/shape' type=Const>,\n",
       " <tf.Operation 'truncated_normal_1/mean' type=Const>,\n",
       " <tf.Operation 'truncated_normal_1/stddev' type=Const>,\n",
       " <tf.Operation 'truncated_normal_1/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'truncated_normal_1/mul' type=Mul>,\n",
       " <tf.Operation 'truncated_normal_1' type=Add>,\n",
       " <tf.Operation 'Variable_2' type=VariableV2>,\n",
       " <tf.Operation 'Variable_2/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_2/read' type=Identity>,\n",
       " <tf.Operation 'ones_1/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'ones_1/Const' type=Const>,\n",
       " <tf.Operation 'ones_1' type=Fill>,\n",
       " <tf.Operation 'truediv_1/y' type=Const>,\n",
       " <tf.Operation 'truediv_1' type=RealDiv>,\n",
       " <tf.Operation 'Variable_3' type=VariableV2>,\n",
       " <tf.Operation 'Variable_3/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_3/read' type=Identity>,\n",
       " <tf.Operation 'truncated_normal_2/shape' type=Const>,\n",
       " <tf.Operation 'truncated_normal_2/mean' type=Const>,\n",
       " <tf.Operation 'truncated_normal_2/stddev' type=Const>,\n",
       " <tf.Operation 'truncated_normal_2/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'truncated_normal_2/mul' type=Mul>,\n",
       " <tf.Operation 'truncated_normal_2' type=Add>,\n",
       " <tf.Operation 'Variable_4' type=VariableV2>,\n",
       " <tf.Operation 'Variable_4/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_4/read' type=Identity>,\n",
       " <tf.Operation 'ones_2/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'ones_2/Const' type=Const>,\n",
       " <tf.Operation 'ones_2' type=Fill>,\n",
       " <tf.Operation 'truediv_2/y' type=Const>,\n",
       " <tf.Operation 'truediv_2' type=RealDiv>,\n",
       " <tf.Operation 'Variable_5' type=VariableV2>,\n",
       " <tf.Operation 'Variable_5/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_5/read' type=Identity>,\n",
       " <tf.Operation 'Conv2D' type=Conv2D>,\n",
       " <tf.Operation 'add' type=Add>,\n",
       " <tf.Operation 'Relu' type=Relu>,\n",
       " <tf.Operation 'MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'Conv2D_1' type=Conv2D>,\n",
       " <tf.Operation 'add_1' type=Add>,\n",
       " <tf.Operation 'Relu_1' type=Relu>,\n",
       " <tf.Operation 'MaxPool_1' type=MaxPool>,\n",
       " <tf.Operation 'Reshape_1/shape' type=Const>,\n",
       " <tf.Operation 'Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'MatMul' type=MatMul>,\n",
       " <tf.Operation 'add_2' type=Add>,\n",
       " <tf.Operation 'Softmax' type=Softmax>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/labels_stop_gradient' type=StopGradient>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Rank' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Shape' type=Shape>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Rank_1' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Shape_1' type=Shape>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Sub/y' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Sub' type=Sub>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice/begin' type=Pack>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice/size' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice' type=Slice>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/concat/values_0' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/concat/axis' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/concat' type=ConcatV2>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Reshape' type=Reshape>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Rank_2' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Shape_2' type=Shape>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Sub_1/y' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Sub_1' type=Sub>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice_1/begin' type=Pack>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice_1/size' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice_1' type=Slice>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/concat_1/values_0' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/concat_1/axis' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/concat_1' type=ConcatV2>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg' type=SoftmaxCrossEntropyWithLogits>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Sub_2/y' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Sub_2' type=Sub>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice_2/begin' type=Const>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice_2/size' type=Pack>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Slice_2' type=Slice>,\n",
       " <tf.Operation 'softmax_cross_entropy_with_logits_sg/Reshape_2' type=Reshape>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'Mean' type=Mean>,\n",
       " <tf.Operation 'ArgMax/dimension' type=Const>,\n",
       " <tf.Operation 'ArgMax' type=ArgMax>,\n",
       " <tf.Operation 'ArgMax_1/dimension' type=Const>,\n",
       " <tf.Operation 'ArgMax_1' type=ArgMax>,\n",
       " <tf.Operation 'Equal' type=Equal>,\n",
       " <tf.Operation 'Cast' type=Cast>,\n",
       " <tf.Operation 'Const_1' type=Const>,\n",
       " <tf.Operation 'Mean_1' type=Mean>,\n",
       " <tf.Operation 'gradients/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/grad_ys_0' type=Const>,\n",
       " <tf.Operation 'gradients/Fill' type=Fill>,\n",
       " <tf.Operation 'gradients/Mean_grad/Reshape/shape' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/Mean_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Mean_grad/Tile' type=Tile>,\n",
       " <tf.Operation 'gradients/Mean_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/Mean_grad/Shape_2' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Const' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Prod' type=Prod>,\n",
       " <tf.Operation 'gradients/Mean_grad/Const_1' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Prod_1' type=Prod>,\n",
       " <tf.Operation 'gradients/Mean_grad/Maximum/y' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Maximum' type=Maximum>,\n",
       " <tf.Operation 'gradients/Mean_grad/floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'gradients/Mean_grad/Cast' type=Cast>,\n",
       " <tf.Operation 'gradients/Mean_grad/truediv' type=RealDiv>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg/Reshape_2_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg/Reshape_2_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/zeros_like' type=ZerosLike>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/LogSoftmax' type=LogSoftmax>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/Neg' type=Neg>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1/dim' type=Const>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1' type=ExpandDims>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_2_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/add_2_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/add_2_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/add_2_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/add_2_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_2_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/add_2_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_2_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/add_2_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/add_2_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/MatMul_grad/MatMul' type=MatMul>,\n",
       " <tf.Operation 'gradients/MatMul_grad/MatMul_1' type=MatMul>,\n",
       " <tf.Operation 'gradients/MatMul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/MatMul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/MatMul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/Reshape_1_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Reshape_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/MaxPool_1_grad/MaxPoolGrad' type=MaxPoolGrad>,\n",
       " <tf.Operation 'gradients/Relu_1_grad/ReluGrad' type=ReluGrad>,\n",
       " <tf.Operation 'gradients/add_1_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/add_1_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/add_1_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/add_1_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/add_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_1_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/add_1_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_1_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/add_1_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/add_1_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/Conv2D_1_grad/ShapeN' type=ShapeN>,\n",
       " <tf.Operation 'gradients/Conv2D_1_grad/Const' type=Const>,\n",
       " <tf.Operation 'gradients/Conv2D_1_grad/Conv2DBackpropInput' type=Conv2DBackpropInput>,\n",
       " <tf.Operation 'gradients/Conv2D_1_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>,\n",
       " <tf.Operation 'gradients/Conv2D_1_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/Conv2D_1_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/Conv2D_1_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>,\n",
       " <tf.Operation 'gradients/Relu_grad/ReluGrad' type=ReluGrad>,\n",
       " <tf.Operation 'gradients/add_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/add_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/add_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/add_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/add_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/add_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/add_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/add_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/Conv2D_grad/ShapeN' type=ShapeN>,\n",
       " <tf.Operation 'gradients/Conv2D_grad/Const' type=Const>,\n",
       " <tf.Operation 'gradients/Conv2D_grad/Conv2DBackpropInput' type=Conv2DBackpropInput>,\n",
       " <tf.Operation 'gradients/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>,\n",
       " <tf.Operation 'gradients/Conv2D_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/Conv2D_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/Conv2D_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'beta1_power/initial_value' type=Const>,\n",
       " <tf.Operation 'beta1_power' type=VariableV2>,\n",
       " <tf.Operation 'beta1_power/Assign' type=Assign>,\n",
       " <tf.Operation 'beta1_power/read' type=Identity>,\n",
       " <tf.Operation 'beta2_power/initial_value' type=Const>,\n",
       " <tf.Operation 'beta2_power' type=VariableV2>,\n",
       " <tf.Operation 'beta2_power/Assign' type=Assign>,\n",
       " <tf.Operation 'beta2_power/read' type=Identity>,\n",
       " <tf.Operation 'Variable/Adam/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'Variable/Adam/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'Variable/Adam/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'Variable/Adam' type=VariableV2>,\n",
       " <tf.Operation 'Variable/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable/Adam/read' type=Identity>,\n",
       " <tf.Operation 'Variable/Adam_1/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'Variable/Adam_1/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'Variable/Adam_1/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'Variable/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'Variable/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'Variable_1/Adam/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'Variable_1/Adam/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'Variable_1/Adam/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'Variable_1/Adam' type=VariableV2>,\n",
       " <tf.Operation 'Variable_1/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_1/Adam/read' type=Identity>,\n",
       " <tf.Operation 'Variable_1/Adam_1/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'Variable_1/Adam_1/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'Variable_1/Adam_1/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'Variable_1/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'Variable_1/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_1/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'Variable_2/Adam/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'Variable_2/Adam/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'Variable_2/Adam/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'Variable_2/Adam' type=VariableV2>,\n",
       " <tf.Operation 'Variable_2/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_2/Adam/read' type=Identity>,\n",
       " <tf.Operation 'Variable_2/Adam_1/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'Variable_2/Adam_1/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'Variable_2/Adam_1/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'Variable_2/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'Variable_2/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_2/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'Variable_3/Adam/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'Variable_3/Adam/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'Variable_3/Adam/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'Variable_3/Adam' type=VariableV2>,\n",
       " <tf.Operation 'Variable_3/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_3/Adam/read' type=Identity>,\n",
       " <tf.Operation 'Variable_3/Adam_1/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'Variable_3/Adam_1/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'Variable_3/Adam_1/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'Variable_3/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'Variable_3/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_3/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'Variable_4/Adam/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'Variable_4/Adam/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'Variable_4/Adam/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'Variable_4/Adam' type=VariableV2>,\n",
       " <tf.Operation 'Variable_4/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_4/Adam/read' type=Identity>,\n",
       " <tf.Operation 'Variable_4/Adam_1/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'Variable_4/Adam_1/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'Variable_4/Adam_1/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'Variable_4/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'Variable_4/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_4/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'Variable_5/Adam/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'Variable_5/Adam/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'Variable_5/Adam/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'Variable_5/Adam' type=VariableV2>,\n",
       " <tf.Operation 'Variable_5/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_5/Adam/read' type=Identity>,\n",
       " <tf.Operation 'Variable_5/Adam_1/Initializer/zeros/shape_as_tensor' type=Const>,\n",
       " <tf.Operation 'Variable_5/Adam_1/Initializer/zeros/Const' type=Const>,\n",
       " <tf.Operation 'Variable_5/Adam_1/Initializer/zeros' type=Fill>,\n",
       " <tf.Operation 'Variable_5/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'Variable_5/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'Variable_5/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'Adam/beta1' type=Const>,\n",
       " <tf.Operation 'Adam/beta2' type=Const>,\n",
       " <tf.Operation 'Adam/epsilon' type=Const>,\n",
       " <tf.Operation 'Adam/update_Variable/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_Variable_1/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_Variable_2/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_Variable_3/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_Variable_4/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_Variable_5/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/mul' type=Mul>,\n",
       " <tf.Operation 'Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'Adam/mul_1' type=Mul>,\n",
       " <tf.Operation 'Adam/Assign_1' type=Assign>,\n",
       " <tf.Operation 'Adam' type=NoOp>,\n",
       " <tf.Operation 'init' type=NoOp>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP: Predicting, Saving, Showcasing strengths and weaknesses "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
